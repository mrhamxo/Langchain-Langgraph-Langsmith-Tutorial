{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7761724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import operator\n",
    "from typing import TypedDict, List, Annotated\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e985c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "    brief: str = Field(..., description=\"What to cover\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43fd7db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    tasks: List[Task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e2dabca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    plan: Plan\n",
    "    # reducer: results from workers get concatenated automatically\n",
    "    sections: Annotated[List[str], operator.add]\n",
    "    final: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b263467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State) -> dict:\n",
    "\n",
    "    plan = llm.with_structured_output(Plan).invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=(\n",
    "                    \"Create a blog plan with 5-7 sections on the following topic.\"\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(content=f\"Topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "    return {\"plan\": plan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fanout(state: State):\n",
    "    return [Send(\"worker\", {\"task\": task, \"topic\": state[\"topic\"], \"plan\": state[\"plan\"]})\n",
    "            for task in state[\"plan\"].tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91c0473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(payload: dict) -> dict:\n",
    "\n",
    "    # payload contains what we sent\n",
    "    task = payload[\"task\"]\n",
    "    topic = payload[\"topic\"]\n",
    "    plan = payload[\"plan\"]\n",
    "\n",
    "    blog_title = plan.blog_title\n",
    "\n",
    "    section_md = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Write one clean Markdown section.\"),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Blog: {blog_title}\\n\"\n",
    "                    f\"Topic: {topic}\\n\\n\"\n",
    "                    f\"Section: {task.title}\\n\"\n",
    "                    f\"Brief: {task.brief}\\n\\n\"\n",
    "                    \"Return only the section content in Markdown.\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    ).content.strip()\n",
    "\n",
    "    return {\"sections\": [section_md]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab5f134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer(state: State) -> dict:\n",
    "\n",
    "    title = state[\"plan\"].blog_title\n",
    "    body = \"\\n\\n\".join(state[\"sections\"]).strip()\n",
    "    \n",
    "    final_md = f\"# {title}\\n\\n{body}\\n\"\n",
    "\n",
    "    # Save to file\n",
    "    filename = \"\".join(c if c.isalnum() or c in (\" \", \"_\", \"-\") else \"\" for c in title)\n",
    "    filename = filename.strip().lower().replace(\" \", \"_\") + \".md\"\n",
    "    Path(filename).write_text(final_md, encoding=\"utf-8\")\n",
    "\n",
    "    return {\"final\": final_md}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3f89f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x19ee0648ad0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = StateGraph(State)\n",
    "graph.add_node(\"orchestrator\", orchestrator)\n",
    "graph.add_node(\"worker\", worker)\n",
    "graph.add_node(\"reducer\", reducer)\n",
    "\n",
    "graph.add_edge(START, \"orchestrator\")\n",
    "graph.add_conditional_edges(\"orchestrator\", fanout, [\"worker\"])\n",
    "graph.add_edge(\"worker\", \"reducer\")\n",
    "graph.add_edge(\"reducer\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000019EDF3B78F0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = graph.compile()\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b9b13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = app.invoke({\"topic\": \"Write a blog on Self Attention\", \"sections\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ca2ddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Demystifying Self-Attention: The Engine Behind Modern NLP\n",
      "\n",
      "## Introduction to Attention Mechanisms\n",
      "\n",
      "In the early days of neural language models, **sequential processing** was the dominant paradigm. Recurrent networks (RNNs, LSTMs, GRUs) read a sentence token‑by‑token, compressing all prior information into a single hidden state. While powerful, this approach suffered from two fundamental drawbacks:\n",
      "\n",
      "1. **Information bottleneck** – a fixed‑size vector must encode everything the model has seen, making it hard to retain long‑range dependencies.  \n",
      "2. **Uniform treatment of context** – each token contributes equally to the hidden state, even though some words (e.g., “not”, proper nouns, or domain‑specific terms) are far more informative for a given prediction.\n",
      "\n",
      "### The Birth of Attention\n",
      "\n",
      "The first breakthrough came in 2014–2015 with the **Bahdanau et al. (2015)** and **Luong et al. (2015)** papers on *neural machine translation*. Their key insight: rather than forcing the decoder to rely on a single compressed representation, let it **learn to focus** on relevant parts of the encoder’s output at each generation step. This “soft” alignment was implemented as a weighted sum of encoder hidden states, where the weights—*attention scores*—were computed dynamically from the current decoder state.\n",
      "\n",
      "The impact was immediate:\n",
      "\n",
      "- **Improved translation quality**, especially for long sentences.  \n",
      "- **Interpretability**, as the attention matrix could be visualized to see which source words influenced each target word.  \n",
      "- **Generalization** to other tasks (image captioning, speech recognition, etc.), establishing attention as a universal building block.\n",
      "\n",
      "### From Sequence‑to‑Sequence to Self‑Attention\n",
      "\n",
      "While the original attention mechanisms operated **between two sequences** (encoder ↔ decoder), researchers soon asked: *What if we let a sequence attend to itself?* This question gave rise to **self‑attention**, first popularized by the **Transformer** architecture (Vaswani et al., 2017).\n",
      "\n",
      "Key reasons self‑attention is a pivotal evolution:\n",
      "\n",
      "| Aspect | Traditional RNN‑based models | Self‑Attention (Transformer) |\n",
      "|--------|------------------------------|------------------------------|\n",
      "| **Parallelism** | Strictly sequential; limited GPU utilization | Fully parallelizable across tokens; massive speedup |\n",
      "| **Long‑range dependencies** | Decay exponentially with distance | Direct, constant‑time connections between any pair of tokens |\n",
      "| **Contextual flexibility** | Fixed receptive field per layer | Dynamic, data‑dependent weighting of all positions |\n",
      "| **Scalability** | Hard to scale depth without vanishing gradients | Layer stacking is straightforward; depth yields richer representations |\n",
      "| **Interpretability** | Implicit, hidden‑state based | Explicit attention maps for every head and layer |\n",
      "\n",
      "Self‑attention thus **replaces recurrence** with a mechanism that directly relates each token to every other token in the same sequence. By learning *query*, *key*, and *value* projections, the model computes attention scores that capture semantic relevance, irrespective of positional distance. This simple yet powerful shift enabled:\n",
      "\n",
      "- **State‑of‑the‑art performance** on a wide range of NLP benchmarks (translation, language modeling, question answering).  \n",
      "- **Foundation models** (BERT, GPT, T5, etc.) that can be pre‑trained on massive corpora and fine‑tuned for diverse downstream tasks.  \n",
      "- **Cross‑modal extensions** (Vision Transformers, multimodal Transformers) that apply the same principle beyond text.\n",
      "\n",
      "In short, attention introduced the idea of **learnable focus**, and self‑attention elevated it to a **universal, scalable engine** that underpins modern deep learning breakthroughs. The rest of this blog will unpack how self‑attention works under the hood and why it has become the cornerstone of today’s NLP revolution.\n",
      "\n",
      "## The Mathematics of Self‑Attention\n",
      "\n",
      "Self‑attention lets a token “look at” every other token in the same sequence and decide how much each one should contribute to its new representation. The process is fully differentiable and can be expressed with a handful of linear algebra operations.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. From Tokens to Query, Key, and Value Vectors  \n",
      "\n",
      "Given an input sequence of \\(n\\) tokens, we first embed each token into a hidden vector \\(\\mathbf{x}_i \\in \\mathbb{R}^{d_{\\text{model}}}\\).  \n",
      "Three learned weight matrices project \\(\\mathbf{x}_i\\) into three distinct spaces:\n",
      "\n",
      "\\[\n",
      "\\begin{aligned}\n",
      "\\mathbf{q}_i &= \\mathbf{W}_Q \\mathbf{x}_i \\qquad &(\\text{query})\\\\\n",
      "\\mathbf{k}_i &= \\mathbf{W}_K \\mathbf{x}_i \\qquad &(\\text{key})\\\\\n",
      "\\mathbf{v}_i &= \\mathbf{W}_V \\mathbf{x}_i \\qquad &(\\text{value})\n",
      "\\end{aligned}\n",
      "\\]\n",
      "\n",
      "- \\(\\mathbf{W}_Q, \\mathbf{W}_K, \\mathbf{W}_V \\in \\mathbb{R}^{d_k \\times d_{\\text{model}}}\\) (often \\(d_k = d_v = d_{\\text{model}}/h\\) for multi‑head attention).  \n",
      "- \\(\\mathbf{q}_i, \\mathbf{k}_i, \\mathbf{v}_i \\in \\mathbb{R}^{d_k}\\).\n",
      "\n",
      "*Intuition*:  \n",
      "- **Query** asks “what am I looking for?”.  \n",
      "- **Key** encodes “what each token has to offer”.  \n",
      "- **Value** holds the actual content that will be mixed together.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Scaled Dot‑Product Attention  \n",
      "\n",
      "For a single query \\(\\mathbf{q}_i\\), we compare it to every key \\(\\mathbf{k}_j\\) using a dot product:\n",
      "\n",
      "\\[\n",
      "\\alpha_{ij} = \\frac{\\mathbf{q}_i^\\top \\mathbf{k}_j}{\\sqrt{d_k}}\n",
      "\\]\n",
      "\n",
      "The scaling factor \\(\\sqrt{d_k}\\) prevents the dot products from growing too large when \\(d_k\\) is high, which would push the softmax into regions with vanishing gradients.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Softmax – Turning Scores into Probabilities  \n",
      "\n",
      "The raw scores \\(\\alpha_{ij}\\) are turned into attention weights via a softmax over the *key* dimension:\n",
      "\n",
      "\\[\n",
      "\\hat{\\alpha}_{ij}= \\operatorname{softmax}_j(\\alpha_{ij})=\n",
      "\\frac{\\exp(\\alpha_{ij})}{\\sum_{l=1}^{n}\\exp(\\alpha_{il})}\n",
      "\\]\n",
      "\n",
      "\\[\n",
      "\\hat{\\alpha}_{i1},\\dots,\\hat{\\alpha}_{in}\\;\\; \\text{form a probability distribution for query } i.\n",
      "\\]\n",
      "\n",
      "*Visual aid (textual)*  \n",
      "\n",
      "```\n",
      "query_i  ──►  dot‑product  ◄── key_1\n",
      "   │                               │\n",
      "   │                               ▼\n",
      "   └─►  scores α_i1 … α_in  ──► softmax  ──► weights w_i1 … w_in\n",
      "```\n",
      "\n",
      "Each weight \\( \\hat{\\alpha}_{ij} \\) tells us **how much token j should influence token i**.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Weighted Sum of Values  \n",
      "\n",
      "Finally, we aggregate the values using the attention weights:\n",
      "\n",
      "\\[\n",
      "\\mathbf{z}_i = \\sum_{j=1}^{n} \\hat{\\alpha}_{ij}\\,\\mathbf{v}_j\n",
      "\\]\n",
      "\n",
      "\\(\\mathbf{z}_i\\) is the *output* representation for token i, now enriched with context from the whole sequence.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Matrix‑Friendly Form  \n",
      "\n",
      "When we stack all queries, keys, and values into matrices  \n",
      "\n",
      "\\[\n",
      "\\mathbf{Q} = [\\mathbf{q}_1^\\top;\\dots;\\mathbf{q}_n^\\top] \\in \\mathbb{R}^{n\\times d_k},\n",
      "\\quad\n",
      "\\mathbf{K} = [\\mathbf{k}_1^\\top;\\dots;\\mathbf{k}_n^\\top] \\in \\mathbb{R}^{n\\times d_k},\n",
      "\\quad\n",
      "\\mathbf{V} = [\\mathbf{v}_1^\\top;\\dots;\\mathbf{v}_n^\\top] \\in \\mathbb{R}^{n\\times d_v},\n",
      "\\]\n",
      "\n",
      "the whole attention step collapses to a single line:\n",
      "\n",
      "\\[\n",
      "\\boxed{\\;\\operatorname{Attention}(\\mathbf{Q},\\mathbf{K},\\mathbf{V})\n",
      "= \\operatorname{softmax}\\!\\Big(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}}\\Big)\\mathbf{V}\\;}\n",
      "\\]\n",
      "\n",
      "*Why it works*:  \n",
      "- \\(\\mathbf{Q}\\mathbf{K}^\\top\\) computes **all pairwise similarities** in one matrix multiplication.  \n",
      "- Softmax normalizes each row, giving a distribution per query.  \n",
      "- Multiplying by \\(\\mathbf{V}\\) mixes the values according to those distributions.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Quick Intuitive Diagram  \n",
      "\n",
      "```\n",
      "   Queries (Q)          Keys (K)           Values (V)\n",
      "   ────────►  Q·Kᵀ  ───────►  Softmax  ───────►  Weighted sum\n",
      "   (n×d_k)                (n×n)               (n×d_v)\n",
      "        │                 │                     │\n",
      "        ▼                 ▼                     ▼\n",
      "   ───────────────────────────────────────────────────────►  Output Z (n×d_v)\n",
      "```\n",
      "\n",
      "Each row of the final matrix \\( \\mathbf{Z} \\) corresponds to a token that has “paid attention” to every other token, weighted by how relevant those tokens are to it.\n",
      "\n",
      "---\n",
      "\n",
      "### 7. Take‑away  \n",
      "\n",
      "- **Query‑Key dot‑product** measures relevance.  \n",
      "- **Scaling** stabilizes gradients.  \n",
      "- **Softmax** turns relevance into a probability distribution.  \n",
      "- **Weighted sum of Values** injects the contextual information.  \n",
      "\n",
      "Understanding these equations demystifies the “engine” that powers modern transformers, and sets the stage for extensions such as multi‑head attention, relative positional encodings, and efficient approximations.\n",
      "\n",
      "## Self‑Attention in Transformer Architecture\n",
      "\n",
      "The Transformer block is built around a **self‑attention** core that lets every token attend to every other token in the same sequence. Below is a step‑by‑step view of how the self‑attention mechanism is wired together with the other essential components: multi‑head attention, residual (skip) connections, and layer normalization.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. The High‑Level Blueprint\n",
      "\n",
      "```\n",
      "Input → [Multi‑Head Self‑Attention] → Add & Norm → [Feed‑Forward] → Add & Norm → Output\n",
      "```\n",
      "\n",
      "| Component | Purpose |\n",
      "|-----------|---------|\n",
      "| **Multi‑Head Self‑Attention** | Computes attention from multiple representation sub‑spaces in parallel. |\n",
      "| **Add & Norm** (Residual + LayerNorm) | Stabilizes training by preserving the original signal and normalizing activations. |\n",
      "| **Position‑wise Feed‑Forward** | Introduces non‑linearity and mixes information across dimensions. |\n",
      "| **Add & Norm** (second) | Same stabilizing role after the feed‑forward stage. |\n",
      "\n",
      "The block is repeated \\(N\\) times (e.g., 6–12 layers) to form the full encoder or decoder stack.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Inside the Multi‑Head Self‑Attention Layer\n",
      "\n",
      "1. **Linear Projections**  \n",
      "   For each head \\(h\\) we learn three weight matrices:\n",
      "   \\[\n",
      "   Q^{(h)} = XW_Q^{(h)}, \\quad\n",
      "   K^{(h)} = XW_K^{(h)}, \\quad\n",
      "   V^{(h)} = XW_V^{(h)}\n",
      "   \\]\n",
      "   where \\(X\\in\\mathbb{R}^{L\\times d_{\\text{model}}}\\) is the input sequence (length \\(L\\)).\n",
      "\n",
      "2. **Scaled Dot‑Product Attention** (per head)  \n",
      "   \\[\n",
      "   \\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
      "   \\]\n",
      "   This yields a context matrix \\(Z^{(h)}\\in\\mathbb{R}^{L\\times d_v}\\).\n",
      "\n",
      "3. **Concatenation & Final Linear Projection**  \n",
      "   \\[\n",
      "   Z = \\text{Concat}(Z^{(1)},\\dots,Z^{(H)})W_O\n",
      "   \\]\n",
      "   where \\(H\\) is the number of heads and \\(W_O\\) projects the concatenated heads back to \\(d_{\\text{model}}\\).\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Residual Connection + Layer Normalization\n",
      "\n",
      "After the multi‑head attention we add the original input (a *skip connection*) and then normalize:\n",
      "\n",
      "\\[\n",
      "\\text{Output}_1 = \\text{LayerNorm}(X + Z)\n",
      "\\]\n",
      "\n",
      "- **Why residual?**  \n",
      "  It lets gradients flow directly through the network, mitigating the vanishing‑gradient problem and allowing deeper stacks.\n",
      "\n",
      "- **Why LayerNorm?**  \n",
      "  Normalizing across the feature dimension stabilizes the hidden‑state distribution, making training faster and more robust than batch‑norm in sequence models.\n",
      "\n",
      "The same pattern repeats after the feed‑forward sub‑layer:\n",
      "\n",
      "\\[\n",
      "\\text{Output}_2 = \\text{LayerNorm}(\\text{Output}_1 + \\text{FFN}(\\text{Output}_1))\n",
      "\\]\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Putting It All Together (Pseudo‑code)\n",
      "\n",
      "```python\n",
      "def transformer_block(X):\n",
      "    # ---- Multi‑Head Self‑Attention ----\n",
      "    Z = multi_head_self_attention(X)          # shape: (L, d_model)\n",
      "\n",
      "    # ---- First Residual + LayerNorm ----\n",
      "    X1 = layer_norm(X + Z)\n",
      "\n",
      "    # ---- Position‑wise Feed‑Forward ----\n",
      "    FF = feed_forward(X1)                     # shape: (L, d_model)\n",
      "\n",
      "    # ---- Second Residual + LayerNorm ----\n",
      "    out = layer_norm(X1 + FF)\n",
      "\n",
      "    return out\n",
      "```\n",
      "\n",
      "Each call to `transformer_block` corresponds to one layer of the encoder (or decoder, with an additional cross‑attention sub‑layer).\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Visual Summary\n",
      "\n",
      "```\n",
      "          ┌─────────────────────────────────────┐\n",
      "          │          Input Embeddings X          │\n",
      "          └─────────────────────┬───────────────┘\n",
      "                                │\n",
      "               ┌────────────────▼─────────────────┐\n",
      "               │   Multi‑Head Self‑Attention (MHA) │\n",
      "               └─────────────────┬────────────────┘\n",
      "                                 │\n",
      "                 ┌───────────────▼───────────────┐\n",
      "                 │   Add (skip) + LayerNorm (1)   │\n",
      "                 └───────────────┬───────────────┘\n",
      "                                 │\n",
      "               ┌─────────────────▼─────────────────┐\n",
      "               │   Position‑wise Feed‑Forward (FF) │\n",
      "               └─────────────────┬────────────────┘\n",
      "                                 │\n",
      "                 ┌───────────────▼───────────────┐\n",
      "                 │   Add (skip) + LayerNorm (2)   │\n",
      "                 └───────────────┬───────────────┘\n",
      "                                 │\n",
      "                               Output\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### 6. Key Takeaways\n",
      "\n",
      "- **Self‑attention** is the engine that mixes token information globally.\n",
      "- **Multi‑head** splits this mixing into several sub‑spaces, enriching the representation.\n",
      "- **Residual connections** preserve the original signal and enable deep stacking.\n",
      "- **Layer normalization** keeps the training dynamics stable after each sub‑layer.\n",
      "\n",
      "Understanding how these pieces interlock demystifies why Transformers have become the de‑facto backbone of modern NLP.\n",
      "\n",
      "## Advantages Over Traditional RNN/CNN Approaches  \n",
      "\n",
      "| Aspect | Self‑Attention (Transformer) | RNN (LSTM/GRU) | CNN (Temporal Conv.) |\n",
      "|--------|-----------------------------|----------------|----------------------|\n",
      "| **Computational Complexity** | **O(n²·d)** per layer (where *n* = sequence length, *d* = hidden dimension). With efficient approximations (e.g., Linformer, Performer) it can be reduced to **O(n·d)**. | **O(n·d²)** (sequential recurrence) – each step depends on the previous hidden state, leading to quadratic cost in hidden size. | **O(k·n·d)** where *k* is kernel size. Limited receptive field; to cover long contexts *k* must grow, increasing cost. |\n",
      "| **Parallelism** | Fully parallel across all positions because each token attends to every other token simultaneously. | Inherently sequential; cannot compute time step *t* before *t‑1* is known. | Parallel across positions within a layer, but depth is needed to enlarge receptive field, limiting overall speed. |\n",
      "| **Long‑Range Dependency Capture** | Direct pairwise interactions allow any two tokens to influence each other in a single layer, regardless of distance. | Dependencies decay with depth; gradients vanish/explode, making very long‑range relations hard to learn. | Fixed receptive field; capturing distant relations requires many stacked layers, which can be inefficient. |\n",
      "| **Empirical Performance on Benchmarks** | - **Machine Translation (WMT)**: Transformers achieve BLEU scores 2–3 points higher than LSTM baselines. <br> - **Language Modeling (WikiText‑103, PTB)**: Per‑token perplexities drop 10–20 % compared to LSTM/CNN models. <br> - **GLUE / SuperGLUE**: Pre‑trained BERT/RoBERTa (self‑attention) consistently outperforms CNN/RNN‑based encoders across all tasks. | Competitive on small‑scale tasks but lag behind Transformers on large‑scale data; performance plateaus as sequence length grows. | Good for local pattern extraction (e.g., character‑level tasks) but underperforms on tasks requiring global context (e.g., document‑level QA). |\n",
      "| **Scalability & Model Size** | Easy to scale depth and width; performance improves predictably with more layers/heads. | Scaling depth quickly leads to optimization difficulties (gradient vanishing) and higher memory due to hidden states. | Scaling width (more channels) increases compute linearly but does not directly improve global context handling. |\n",
      "\n",
      "### Key Takeaways\n",
      "- **Parallelism** is the single biggest practical advantage: training time drops from days (RNN) to hours (Transformer) on the same hardware.\n",
      "- **Direct global interactions** enable the model to learn long‑range patterns without deep stacks, which is why Transformers dominate tasks like machine translation, summarization, and large‑scale language modeling.\n",
      "- **Empirical evidence** across diverse benchmarks consistently shows self‑attention models achieving higher accuracy / lower perplexity with comparable or lower training cost when hardware parallelism is exploited.\n",
      "\n",
      "## Practical Implementation Tips\n",
      "\n",
      "Below are battle‑tested patterns you can copy‑paste into your projects. The snippets cover both **PyTorch** and **TensorFlow**, show how to apply **masking**, deal with **variable‑length sequences**, and include a few **memory‑efficient tricks** for large batches.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Core Self‑Attention Block (PyTorch)\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "\n",
      "class SelfAttention(nn.Module):\n",
      "    def __init__(self, dim, heads=8, dropout=0.1):\n",
      "        super().__init__()\n",
      "        self.dim   = dim\n",
      "        self.heads = heads\n",
      "        self.head_dim = dim // heads\n",
      "        assert self.head_dim * heads == dim, \"dim must be divisible by heads\"\n",
      "\n",
      "        self.qkv = nn.Linear(dim, dim * 3, bias=False)   # query, key, value together\n",
      "        self.out = nn.Linear(dim, dim)\n",
      "        self.dropout = nn.Dropout(dropout)\n",
      "\n",
      "    def forward(self, x, mask=None):\n",
      "        \"\"\"\n",
      "        x    : (B, T, dim)   – batch, time, embedding\n",
      "        mask : (B, T) or (B, 1, T) – 1 for keep, 0 for ignore\n",
      "        \"\"\"\n",
      "        B, T, _ = x.size()\n",
      "\n",
      "        # (B, T, 3*dim) → (3, B, heads, T, head_dim)\n",
      "        qkv = self.qkv(x).reshape(B, T, 3, self.heads, self.head_dim)\n",
      "        q, k, v = qkv.unbind(dim=2)                     # each: (B, heads, T, head_dim)\n",
      "\n",
      "        # Scaled dot‑product\n",
      "        scores = torch.einsum('bhqd,bhkd->bhqk', q, k)   # (B, heads, T, T)\n",
      "        scores = scores / (self.head_dim ** 0.5)\n",
      "\n",
      "        # ------- Masking -------\n",
      "        if mask is not None:\n",
      "            # mask shape → (B, 1, 1, T) so it broadcasts over heads & query positions\n",
      "            mask = mask[:, None, None, :]               # add axes\n",
      "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
      "\n",
      "        attn = F.softmax(scores, dim=-1)                 # (B, heads, T, T)\n",
      "        attn = self.dropout(attn)\n",
      "\n",
      "        # Weighted sum of values\n",
      "        context = torch.einsum('bhqk,bhkd->bhqd', attn, v)  # (B, heads, T, head_dim)\n",
      "        context = context.contiguous().view(B, T, self.dim)  # merge heads\n",
      "\n",
      "        return self.out(context)\n",
      "```\n",
      "\n",
      "**Key points**\n",
      "\n",
      "| Feature | How it’s handled |\n",
      "|---------|-----------------|\n",
      "| **Mask shape** | Accepts `(B, T)` (e.g., padding mask) and expands automatically. |\n",
      "| **Variable length** | Pass a boolean mask that marks real tokens (`1`) vs padding (`0`). |\n",
      "| **Speed** | `torch.einsum` is often faster than explicit `matmul` for the 4‑D pattern. |\n",
      "| **Numerical stability** | `float('-inf')` before softmax guarantees masked positions get zero probability. |\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Core Self‑Attention Block (TensorFlow 2 / Keras)\n",
      "\n",
      "```python\n",
      "import tensorflow as tf\n",
      "from tensorflow.keras import layers\n",
      "\n",
      "class SelfAttention(tf.keras.layers.Layer):\n",
      "    def __init__(self, dim, heads=8, dropout=0.1):\n",
      "        super().__init__()\n",
      "        self.dim   = dim\n",
      "        self.heads = heads\n",
      "        self.head_dim = dim // heads\n",
      "        assert self.head_dim * heads == dim\n",
      "\n",
      "        self.qkv   = layers.Dense(dim * 3, use_bias=False)\n",
      "        self.out   = layers.Dense(dim)\n",
      "        self.dropout = layers.Dropout(dropout)\n",
      "\n",
      "    def call(self, x, mask=None, training=False):\n",
      "        \"\"\"\n",
      "        x    : (B, T, dim)\n",
      "        mask : (B, T) – 1 for keep, 0 for ignore\n",
      "        \"\"\"\n",
      "        B, T, _ = tf.unstack(tf.shape(x))\n",
      "\n",
      "        # (B, T, 3*dim) → (B, T, 3, heads, head_dim)\n",
      "        qkv = tf.reshape(self.qkv(x), (B, T, 3, self.heads, self.head_dim))\n",
      "        q, k, v = tf.unstack(qkv, axis=2)          # each: (B, T, heads, head_dim)\n",
      "\n",
      "        # Move heads before time for efficient matmul\n",
      "        q = tf.transpose(q, perm=[0, 2, 1, 3])      # (B, heads, T, head_dim)\n",
      "        k = tf.transpose(k, perm=[0, 2, 1, 3])\n",
      "        v = tf.transpose(v, perm=[0, 2, 1, 3])\n",
      "\n",
      "        # Scaled dot‑product\n",
      "        scores = tf.matmul(q, k, transpose_b=True)  # (B, heads, T, T)\n",
      "        scores = scores / tf.math.sqrt(tf.cast(self.head_dim, tf.float32))\n",
      "\n",
      "        # ------- Masking -------\n",
      "        if mask is not None:\n",
      "            mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], tf.float32)  # (B,1,1,T)\n",
      "            scores += (1.0 - mask) * -1e9   # large negative number\n",
      "\n",
      "        attn = tf.nn.softmax(scores, axis=-1)\n",
      "        attn = self.dropout(attn, training=training)\n",
      "\n",
      "        # Weighted sum\n",
      "        context = tf.matmul(attn, v)            # (B, heads, T, head_dim)\n",
      "        context = tf.transpose(context, perm=[0, 2, 1, 3])\n",
      "        context = tf.reshape(context, (B, T, self.dim))\n",
      "\n",
      "        return self.out(context)\n",
      "```\n",
      "\n",
      "**Masking notes**\n",
      "\n",
      "* TensorFlow’s `tf.nn.softmax` automatically treats `-1e9` as “zero probability”.  \n",
      "* The mask is broadcasted to `(B, 1, 1, T)` so it works for any number of heads.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Handling Variable‑Length Batches\n",
      "\n",
      "1. **Padding + Mask Generation**  \n",
      "   ```python\n",
      "   # Example (PyTorch)\n",
      "   lengths = torch.tensor([7, 5, 3])                # true lengths in the batch\n",
      "   max_len = lengths.max()\n",
      "   mask = torch.arange(max_len)[None, :] < lengths[:, None]   # (B, max_len)\n",
      "   ```\n",
      "   The `mask` can be passed directly to the attention modules above.\n",
      "\n",
      "2. **Packed Sequences (PyTorch only)** – If you want to avoid the full `(T,T)` attention matrix for extremely long sequences, you can use *block‑sparse* tricks (see next section) or fall back to RNN‑style `torch.nn.utils.rnn.pack_padded_sequence`. However, true self‑attention still needs the full pairwise matrix, so packing is rarely used for the core attention; it’s more useful for the feed‑forward layers that follow.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Memory‑Efficient Tricks\n",
      "\n",
      "| Trick | When to use | Implementation tip |\n",
      "|-------|-------------|--------------------|\n",
      "| **Chunked attention** (a.k.a. *sliding‑window* or *local* attention) | Sequences > 4‑5k tokens | Split the sequence into overlapping windows of size `W`; compute attention inside each window, then concatenate. |\n",
      "| **FlashAttention / xFormers** | GPU with CUDA ≥ 11.6 | Replace the `einsum`/`matmul` block with `flash_attn_unpadded` from `flash-attn` library – it reduces memory from O(T²) to O(T·head_dim). |\n",
      "| **Mixed‑precision** | Any modern GPU | Wrap the forward pass with `torch.autocast('cuda')` (PyTorch) or `tf.keras.mixed_precision.Policy('mixed_float16')`. |\n",
      "| **Key‑Value caching (inference)** | Autoregressive generation | Store past `k` and `v` tensors; only compute new queries for the latest token and concatenate. |\n",
      "| **Sparse attention matrices** | Long documents (e.g., 10k+ tokens) | Use libraries like `torch-sparse` or `sparse-attention` to build a binary mask that forces most entries to `-inf` before softmax. |\n",
      "\n",
      "**FlashAttention example (PyTorch)**\n",
      "\n",
      "```python\n",
      "from flash_attn import flash_attn_unpadded_qkvpacked_func\n",
      "\n",
      "def forward_flash(self, x, mask=None):\n",
      "    B, T, _ = x.shape\n",
      "    qkv = self.qkv(x)                     # (B, T, 3*dim)\n",
      "    qkv = qkv.view(B, T, 3, self.heads, self.head_dim)\n",
      "    q, k, v = qkv.unbind(dim=2)           # (B, T, heads, head_dim)\n",
      "\n",
      "    # FlashAttention expects (B*heads, T, head_dim)\n",
      "    q = q.reshape(-1, T, self.head_dim)\n",
      "    k = k.reshape(-1, T, self.head_dim)\n",
      "    v = v.reshape(-1, T, self.head_dim)\n",
      "\n",
      "    if mask is not None:\n",
      "        # convert bool mask → int8 where 1 = keep, 0 = ignore\n",
      "        cu_seqlens = torch.arange(0, (B*self.heads+1), device=x.device) * T\n",
      "        # FlashAttention supports a *cumulative* length tensor for variable seqs\n",
      "        # (here we assume all sequences have same length; otherwise build cu_seqlens accordingly)\n",
      "        out = flash_attn_unpadded_qkvpacked_func(q, k, v,\n",
      "                                                cu_seqlens,\n",
      "                                                cu_seqlens,\n",
      "                                                max_seqlen=T,\n",
      "                                                dropout_p=self.dropout.p,\n",
      "                                                softmax_scale=1.0/ (self.head_dim**0.5),\n",
      "                                                causal=False)\n",
      "    else:\n",
      "        out = flash_attn_unpadded_qkvpacked_func(q, k, v,\n",
      "                                                None, None,\n",
      "                                                max_seqlen=T,\n",
      "                                                dropout_p=self.dropout.p,\n",
      "                                                softmax_scale=1.0/ (self.head_dim**0.5),\n",
      "                                                causal=False)\n",
      "\n",
      "    out = out.view(B, T, self.dim)\n",
      "    return self.out(out)\n",
      "```\n",
      "\n",
      "*The function automatically uses the GPU’s shared memory to keep the `(T,T)` matrix off the slower global memory, cutting peak usage by ~30‑50 %.*\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Quick Checklist Before You Ship\n",
      "\n",
      "- [ ] **Mask shape** matches the library’s expectation (`(B, 1, 1, T)` for PyTorch, `(B, 1, 1, T)` for TF).  \n",
      "- [ ] **Dropout** is only applied **after** softmax (not before).  \n",
      "- [ ] **Scaling factor** is `1/√head_dim`.  \n",
      "- [ ] **Mixed‑precision** policy is enabled **once** at model construction, not per‑layer.  \n",
      "- [ ] **Causal flag** (`causal=True`) is set for decoder‑only models (e.g., language generation).  \n",
      "- [ ] **Memory profiling** (`torch.cuda.max_memory_allocated()` or TF’s profiler) shows the attention block as the dominant consumer; consider FlashAttention or windowed attention if it exceeds your GPU budget.\n",
      "\n",
      "---\n",
      "\n",
      "With these snippets and tricks, you can drop a robust self‑attention layer into any transformer‑style model while keeping the code readable, the training fast, and the GPU memory under control. Happy coding!\n",
      "\n",
      "## Applications and Future Directions\n",
      "\n",
      "### Real‑world Deployments  \n",
      "\n",
      "- **Large language models (LLMs)** – From GPT‑4 to LLaMA, self‑attention enables the scaling of parameters and the ability to capture long‑range dependencies, powering chatbots, code assistants, and automated content creation.  \n",
      "- **Vision Transformers (ViT)** – By treating image patches as tokens, self‑attention replaces convolutional kernels, delivering state‑of‑the‑art performance on classification, object detection, and segmentation tasks.  \n",
      "- **Multimodal models** – Architectures such as CLIP, Flamingo, and GPT‑4 Vision fuse text, images, audio, and video through shared attention layers, allowing a single model to understand and generate across modalities.  \n",
      "- **Retrieval‑augmented generation** – Retrieval‑augmented LLMs (e.g., RAG, Atlas) use attention to blend external knowledge bases with generated text, improving factuality and domain specificity.  \n",
      "- **Speech and time‑series analysis** – Transformers with self‑attention have supplanted recurrent networks in speech recognition, music generation, and forecasting, thanks to their parallelism and ability to model global temporal patterns.  \n",
      "\n",
      "### Emerging Research Trends  \n",
      "\n",
      "- **Sparse attention** – Techniques such as BigBird, Longformer, and Routing Transformer restrict attention to a subset of tokens (local windows, global tokens, or learned patterns), reducing the quadratic cost to linear or near‑linear while preserving performance on long documents.  \n",
      "- **Linearized/Kernelized attention** – Methods like Performer, Linear Transformer, and FlashAttention approximate the softmax kernel with feature maps, achieving true \\(O(N)\\) complexity and enabling real‑time inference on very long sequences.  \n",
      "- **Adaptive computation** – Dynamic attention mechanisms decide, on the fly, how many and which tokens to attend to, balancing accuracy and latency for edge devices.  \n",
      "- **Neural architecture search for attention** – Automated discovery of novel attention patterns (e.g., mixed dense‑sparse hybrids) is accelerating the design of task‑specific Transformers.  \n",
      "- **Explainability and interpretability** – New visualisation and attribution tools are being built to dissect attention maps, helping practitioners debug models and comply with emerging AI regulations.  \n",
      "\n",
      "These advances are converging toward **efficient, scalable, and multimodal** Transformers that can be deployed from cloud datacenters to on‑device processors, widening the impact of self‑attention across every corner of AI.\n",
      "\n",
      "## Conclusion & Further Reading\n",
      "\n",
      "### Key Takeaways\n",
      "- **Self‑attention** lets each token weigh the relevance of every other token, enabling models to capture long‑range dependencies without recurrence or convolution.\n",
      "- The **scaled dot‑product** formulation provides stable gradients, while **multi‑head** attention allows the model to attend to information from different representation subspaces simultaneously.\n",
      "- **Positional encodings** inject order information, making the otherwise permutation‑invariant attention mechanism suitable for sequential data.\n",
      "- Self‑attention is the core building block of **Transformer** architectures, powering state‑of‑the‑art models in machine translation, language modeling, vision, and multimodal tasks.\n",
      "- Understanding the **computational trade‑offs** (quadratic cost vs. efficient variants) is essential for scaling self‑attention to longer sequences.\n",
      "\n",
      "### Curated Resources\n",
      "\n",
      "#### Foundational Papers\n",
      "- **“Attention Is All You Need”** – Vaswani et al., 2017  \n",
      "  *The original Transformer paper introducing scaled dot‑product and multi‑head attention.*  \n",
      "  https://arxiv.org/abs/1706.03762\n",
      "- **“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding”** – Devlin et al., 2018  \n",
      "  https://arxiv.org/abs/1810.04805\n",
      "- **“Transformer‑XL: Attentive Language Models Beyond a Fixed‑Length Context”** – Dai et al., 2019  \n",
      "  https://arxiv.org/abs/1901.02860\n",
      "- **“Longformer: The Long‑Document Transformer”** – Beltagy et al., 2020  \n",
      "  https://arxiv.org/abs/2004.05150\n",
      "- **“Efficient Transformers: A Survey”** – Tay et al., 2020 (survey of linear‑complexity variants)  \n",
      "  https://arxiv.org/abs/2009.06732\n",
      "\n",
      "#### Tutorials & Guides\n",
      "- **The Illustrated Transformer** – Jay Alammar (visual walkthrough)  \n",
      "  https://jalammar.github.io/illustrated-transformer/\n",
      "- **Self‑Attention Explained** – Chris Olah (blog post with interactive diagrams)  \n",
      "  https://distill.pub/2019/self-attention/\n",
      "- **Stanford CS224N: Lecture 13 – Transformers & Self‑Attention** (video + slides)  \n",
      "  https://web.stanford.edu/class/cs224n/\n",
      "- **Fast.ai Course – “Attention & Transformers”** (practical notebook‑based tutorial)  \n",
      "  https://course.fast.ai/\n",
      "\n",
      "#### Open‑Source Implementations\n",
      "- **🤗 Hugging Face Transformers** – PyTorch & TensorFlow library with dozens of pretrained models.  \n",
      "  https://github.com/huggingface/transformers\n",
      "- **TensorFlow Addons – MultiHeadAttention layer** – Minimal, production‑ready implementation.  \n",
      "  https://github.com/tensorflow/addons/tree/master/tensorflow_addons/layers\n",
      "- **FlashAttention** – Optimized CUDA kernels for fast, memory‑efficient attention.  \n",
      "  https://github.com/Dao-AILab/flash-attention\n",
      "- **Longformer & BigBird implementations** – Efficient attention for long sequences.  \n",
      "  https://github.com/allenai/longformer  \n",
      "  https://github.com/google-research/bigbird\n",
      "- **OpenAI’s GPT‑4all** – Community‑driven, lightweight inference of large language models.  \n",
      "  https://github.com/nomic-ai/gpt4all\n",
      "\n",
      "#### Hands‑On Projects to Try\n",
      "1. **Build a character‑level language model** using a single‑head self‑attention layer in PyTorch.  \n",
      "2. **Fine‑tune BERT** on a domain‑specific classification task (e.g., sentiment analysis on product reviews).  \n",
      "3. **Experiment with linear‑complexity attention** (e.g., Performer, Linformer) on long‑document summarization.  \n",
      "4. **Integrate visual and textual modalities** by combining a Vision Transformer (ViT) with a language Transformer for image captioning.  \n",
      "\n",
      "---\n",
      "\n",
      "By mastering self‑attention’s mechanics and exploring these resources, you’ll be equipped to both understand existing Transformer breakthroughs and contribute novel improvements to the rapidly evolving NLP landscape. Happy reading and coding!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(out[\"final\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56461113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
